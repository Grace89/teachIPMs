fo0
foreach(i=1:nrow(vars),.options.multicore=mcoptions) %dopar% {  ## calculate the expert range prior  	fo0=paste0(prior.path,species)  	if(!file.exists(fo0)) dir.create(fo0)    fo=paste0(fo0,'/',species,"_prior_",paste(vars[i,],collapse="_"),".grd")    if(file.exists(fo)) return(NULL)    expert=rangeOffset(rdist,dists=dists,                       parms=c(unlist(vars[i,]),upper=1),                       normalize=T,                       verbose=T,                       writeRaster=T,filename=fo,overwrite=T,datatype="FLT4S")  }
points
ex1=raster(range)
plot(range?)
plot(range)
?rasterize
ex1=rasterize(range,env)
tmp1=extract(ex1,points)
table(tmp1)
plot(rdist);plot(range,add=T);plot(points,add=T)
plot(rdist);plot(range,add=T);plot(points,add=T), plot(ex1,add=T)
plot(ex1)
points
values(points)
str(points)
coordinates(points)
tmp1=extract(ex1,points)
tmp1
expert.accuracy=sum(!is.na(tmp1))/length(tmp1)
expert.accuracy
expert.r=rasterize(range,env) #rasterized expert maptmp1=extract(expert.r,points)expert.accuracy=sum(!is.na(tmp1))/length(tmp1)# vars=expand.grid(#   prob=c(expert.accuracy, 0.5, 0.7, 0.9),#   rate=c(0,0.01,0.1,10),#   skew=0.5,#   shift=0,#   stringsAsFactors=F)vars=expand.grid(  prob=c(expert.accuracy,0.5),  rate=c(0.01,.1),  skew=0.5,  shift=0,  stringsAsFactors=F)x=seq(-150,500,len=1000)#'
uvars=unique(vars[,c('prob',"rate","skew","shift")])erd=do.call(rbind,            lapply(1:nrow(uvars),function(i) {              y=logistic(x,parms=unlist(c(lower=0,upper=1,uvars[i,])))                return(cbind.data.frame(group=i,c(uvars[i,]),x=x,y=y))            }))
ggplot(erd, aes(x=x,y=y,linetype=as.factor(skew),colour=as.factor(rate),group=group)) +     geom_vline(aes(0),colour="red")+    geom_line()+    xlab("Prior value (not normalized)")+    xlab("Distance to range edge (km)")
dists=freq(rdist,useNA="no",digits=2)
mcoptions <- list(preschedule=FALSE, set.seed=FALSE)registerDoParallel(7)foreach(i=1:nrow(vars),.options.multicore=mcoptions) %dopar% {  ## calculate the expert range prior  	fo0=paste0(prior.path,species)  	if(!file.exists(fo0)) dir.create(fo0)    fo=paste0(fo0,'/',species,"_prior_",paste(vars[i,],collapse="_"),".grd")    if(file.exists(fo)) return(NULL)    expert=rangeOffset(rdist,dists=dists,                       parms=c(unlist(vars[i,]),upper=1),                       normalize=T,                       verbose=T,                       writeRaster=T,filename=fo,overwrite=T,datatype="FLT4S")  }
fs=list.files(pattern="prior.*grd$",full=T,recursive=T)fs=fs[grepl(species,fs)&!grepl("old",fs)]res= foreach(f=fs,.combine=rbind_list,.packages="dplyr") %dopar% {    dt=raster(f)    mt=metadata(dt)    names(mt$parms)=mt$pnames    tres=data.frame(file=f,species=strsplit(f,"/")[[1]][3],t(mt$parms))    return(tres)    }#
res$dif_0=res$pinside-res$probres$dif_fit=res$pinside_fitbuffer-res$prob
res$dif_0
res$dif_fit
res
fs=list.files(pattern="prior.*grd$",full=T,recursive=T)
fs
?list.files
fs=list.files(paste0(prior.path,species),pattern="prior.*grd$",full=T,recursive=T)
fs
fs=fs[grepl(species,fs)&!grepl("old",fs)]res= foreach(f=fs,.combine=rbind_list,.packages="dplyr") %dopar% {    dt=raster(f)    mt=metadata(dt)    names(mt$parms)=mt$pnames    tres=data.frame(file=f,species=strsplit(f,"/")[[1]][3],t(mt$parms))    return(tres)    }
res
res$dif_0=res$pinside-res$prob
res$dif_fit=res$pinside_fitbuffer-res$prob
(res$dif_fit=res$pinside_fitbuffer-res$prob)
(res$dif_0=res$pinside-res$prob)
res$pinside
res$prob
expert.accuracy=round(sum(!is.na(tmp1))/length(tmp1),2)
vars=expand.grid(  prob=c(expert.accuracy,0.5),  rate=c(0.01,.1),  skew=0.5,  shift=0,  stringsAsFactors=F)x=seq(-150,500,len=1000)#' #' ## Calculate all the curves## ------------------------------------------------------------------------uvars=unique(vars[,c('prob',"rate","skew","shift")])erd=do.call(rbind,            lapply(1:nrow(uvars),function(i) {              y=logistic(x,parms=unlist(c(lower=0,upper=1,uvars[i,])))                return(cbind.data.frame(group=i,c(uvars[i,]),x=x,y=y))            }))  #' #' ## Visualize potential decay parameters## ------------------------------------------------------------------------  ggplot(erd, aes(x=x,y=y,linetype=as.factor(skew),colour=as.factor(rate),group=group)) +     geom_vline(aes(0),colour="red")+    geom_line()+    xlab("Prior value (not normalized)")+    xlab("Distance to range edge (km)")#' #' # Process priors#'           #' ## calculate frequency table of distances## ------------------------------------------------------------------------dists=freq(rdist,useNA="no",digits=2)#knitr::kable(head(dists))#' #' ## Calculate the priors## ----warning=F, results="hide"-------------------------------------------mcoptions <- list(preschedule=FALSE, set.seed=FALSE)registerDoParallel(7)foreach(i=1:nrow(vars),.options.multicore=mcoptions) %dopar% {  ## calculate the expert range prior  	fo0=paste0(prior.path,species)  	if(!file.exists(fo0)) dir.create(fo0)    fo=paste0(fo0,'/',species,"_prior_",paste(vars[i,],collapse="_"),".grd")    if(file.exists(fo)) return(NULL)    expert=rangeOffset(rdist,dists=dists,                       parms=c(unlist(vars[i,]),upper=1),                       normalize=T,                       verbose=T,                       writeRaster=T,filename=fo,overwrite=T,datatype="FLT4S")  }
expert.accuracy
sum(!is.na(tmp1))/length(tmp1)
plot(raster('/Users/ctg/Documents/Data/Expert_Maps/Other_species/Priors/Crocidura fumosa/Crocidura fumosa_prior_0.1_0.1_0.5_0.gri'))
plot(raster('/Users/ctg/Documents/Data/Expert_Maps/Other_species/Priors/Crocidura fumosa/Crocidura fumosa_prior_0.5_0.1_0.5_0.gri'))
ggplot(res,aes(x=prob,y=pinside,               colour=fitbuffer,               size=as.factor(rate),               order=desc(as.factor(rate))))+  geom_abline(intercept = 0, slope = 1,col="grey")+  geom_point(alpha=0.5)+  facet_wrap(~species)
vars=expand.grid(  prob=c(expert.accuracy,0.5),  rate=c(0.01,10),  skew=0.5,  shift=0,  stringsAsFactors=F)x=seq(-150,500,len=1000)#' #' ## Calculate all the curves## ------------------------------------------------------------------------uvars=unique(vars[,c('prob',"rate","skew","shift")])erd=do.call(rbind,            lapply(1:nrow(uvars),function(i) {              y=logistic(x,parms=unlist(c(lower=0,upper=1,uvars[i,])))                return(cbind.data.frame(group=i,c(uvars[i,]),x=x,y=y))            }))  #' #' ## Visualize potential decay parameters## ------------------------------------------------------------------------  ggplot(erd, aes(x=x,y=y,linetype=as.factor(skew),colour=as.factor(rate),group=group)) +     geom_vline(aes(0),colour="red")+    geom_line()+    xlab("Prior value (not normalized)")+    xlab("Distance to range edge (km)")#' #' # Process priors#'           #' ## calculate frequency table of distances## ------------------------------------------------------------------------dists=freq(rdist,useNA="no",digits=2)#knitr::kable(head(dists))#' #' ## Calculate the priors## ----warning=F, results="hide"-------------------------------------------mcoptions <- list(preschedule=FALSE, set.seed=FALSE)registerDoParallel(7)foreach(i=1:nrow(vars),.options.multicore=mcoptions) %dopar% {  ## calculate the expert range prior  	fo0=paste0(prior.path,species)  	if(!file.exists(fo0)) dir.create(fo0)    fo=paste0(fo0,'/',species,"_prior_",paste(vars[i,],collapse="_"),".grd")    if(file.exists(fo)) return(NULL)    expert=rangeOffset(rdist,dists=dists,                       parms=c(unlist(vars[i,]),upper=1),                       normalize=T,                       verbose=T,                       writeRaster=T,filename=fo,overwrite=T,datatype="FLT4S")  }#
#' #' ## Evaluate curves #' #' Calculate which curve parameter combinations were able to get the requested % inside#' ## ------------------------------------------------------------------------fs=list.files(paste0(prior.path,species),pattern="prior.*grd$",full=T,recursive=T)fs=fs[grepl(species,fs)&!grepl("old",fs)]res= foreach(f=fs,.combine=rbind_list,.packages="dplyr") %dopar% {    dt=raster(f)    mt=metadata(dt)    names(mt$parms)=mt$pnames    tres=data.frame(file=f,species=strsplit(f,"/")[[1]][3],t(mt$parms))    return(tres)    }#
(res$dif_0=res$pinside-res$prob)(res$dif_fit=res$pinside_fitbuffer-res$prob)
res
## ------------------------------------------------------------------------ggplot(res,aes(x=prob,y=pinside,               colour=fitbuffer,               size=as.factor(rate),               order=desc(as.factor(rate))))+  geom_abline(intercept = 0, slope = 1,col="grey")+  geom_point(alpha=0.5)+  facet_wrap(~species)
priors=stack(fs)
fs
## build prior table from metadatapriorf=foreach(i=1:nlayers(priors),.combine=rbind_list) %do% {  t1=metadata(priors[[i]])  t2=t1$parms  names(t2)=t1$pnames  return(data.frame(id=i,t(t2)))}#
names(priors)=paste0("prior",priorf$id)#basename(fs[wp])
priors
priorf
plot(proirs)
plot(priors)
plot(priors,zlim=c(1e-7,3e-7))
plot(priors,zlim=c(1.3e-7,2.7e-7))
plot(priors,zlim=c(1.4e-7,2.8e-7))
plot(priors,zlim=c(1.3e-7,2.8e-7))
vars=expand.grid(  prob=c(expert.accuracy,.2,0.5),  rate=c(0.01,.1,10),  skew=0.5,  shift=0,  stringsAsFactors=F)x=seq(-150,500,len=1000)#' #' ## Calculate all the curves## ------------------------------------------------------------------------uvars=unique(vars[,c('prob',"rate","skew","shift")])erd=do.call(rbind,            lapply(1:nrow(uvars),function(i) {              y=logistic(x,parms=unlist(c(lower=0,upper=1,uvars[i,])))                return(cbind.data.frame(group=i,c(uvars[i,]),x=x,y=y))            }))  #' #' ## Visualize potential decay parameters## ------------------------------------------------------------------------  ggplot(erd, aes(x=x,y=y,linetype=as.factor(skew),colour=as.factor(rate),group=group)) +     geom_vline(aes(0),colour="red")+    geom_line()+    xlab("Prior value (not normalized)")+    xlab("Distance to range edge (km)")#' #' # Process priors#'           #' ## calculate frequency table of distances## ------------------------------------------------------------------------dists=freq(rdist,useNA="no",digits=2)#knitr::kable(head(dists))#' #' ## Calculate the priors## ----warning=F, results="hide"-------------------------------------------mcoptions <- list(preschedule=FALSE, set.seed=FALSE)registerDoParallel(7)foreach(i=1:nrow(vars),.options.multicore=mcoptions) %dopar% {  ## calculate the expert range prior  	fo0=paste0(prior.path,species)  	if(!file.exists(fo0)) dir.create(fo0)    fo=paste0(fo0,'/',species,"_prior_",paste(vars[i,],collapse="_"),".grd")    if(file.exists(fo)) return(NULL)    expert=rangeOffset(rdist,dists=dists,                       parms=c(unlist(vars[i,]),upper=1),                       normalize=T,                       verbose=T,                       writeRaster=T,filename=fo,overwrite=T,datatype="FLT4S")  }
fs=list.files(paste0(prior.path,species),pattern="prior.*grd$",full=T,recursive=T)fs=fs[grepl(species,fs)&!grepl("old",fs)]res= foreach(f=fs,.combine=rbind_list,.packages="dplyr") %dopar% {    dt=raster(f)    mt=metadata(dt)    names(mt$parms)=mt$pnames    tres=data.frame(file=f,species=strsplit(f,"/")[[1]][3],t(mt$parms))    return(tres)    }#
(res$dif_0=res$pinside-res$prob)res$dif_fit=res$pinside_fitbuffer-res$prob
res$pinside
res$prob
library(popbio)
?popbio
library(popbio)library(plyr)library(reshape)
andre <-read.csv ("karadat.csv")str(andre)
andre <-read.csv ("/Users/ctg/Dropbox/Projects/ipms/Teaching/Matrix_Models_Intro/Exercises/karadat.csv")
andre <-read.csv ("~/Dropbox/Projects/ipms/Teaching/Matrix_Models_Intro/Exercises/karadat.csv")
str(andre)
levels(andre$CLASS)  # note that you need "dead" as a class for the first year that an individual is
n_options<-ddply (andre, c("YEAR"), function (df)  return(table(df$CLASS)))n_options  # picked starting population vector from 1995, the first year with 9 observed populationsn95<-c(81,31,17,13,11)n=n95  # this must be called "n" for popbio to work its magic
trans<-subset(merge(andre, andre, by = "PLANT_UNQ", sort = FALSE), YEAR.x == YEAR.y - 1)head2(trans)# rename rows and columns to improve clarity (I use the names used by popbio, which are similar to Morris and Doak)# I think I worked out somewhat painfully that you need these particular column names.  rownames(trans) <- 1:nrow(trans)colnames(trans)[1:7] <- c("plant",  "year", "stage", "seeds",  "year2", "fate", "seeds2")head2(trans)#  add individual fertility estimates from the calculations aboveseedlingtrans<-0.00305    # This is the rate at which a seed becomes a J individual (I estimated this elsewhere, see Appendix below)# adding in the number of J individuals produced by each individualtrans$J<- trans$seeds * seedlingtrans   # note that J is not an integer, which is totally fine, its a rate of J productionhead2(trans)
stages <- c("J", "A1", "A2", "A3", "A4")  # you must have a vector named stages in this way for your own clases or stages used # in the "stage" vector of your data frame#################   SET ITERATIONS  ############################it<-100        # set the number of time steps for a deterministic model# Make a demographic projection matrix for each year like so:#     1994  trans94 <-subset(trans, year == 1994, c(plant, stage, fate, J))(proj94<-projection.matrix(trans94, stage, fate, J, sort = stages))  #this gives you a projection matrix for 1994# you can do a simple deterministic projection of the matrix for just this year(p94<-pop.projection(proj94, n, it))(l94<-p94$lambda)   # wow! if we looked only at 1994 based on these estimates the
#     1995  trans95 <-subset(trans, year == 1995, c(plant, stage, fate, J))(proj95<-projection.matrix(trans95, stage, fate, J, sort = stages))#     2011  trans11 <-subset(trans, year == 2011, c(plant, stage, fate, J))(proj11<-projection.matrix(trans11, stage, fate, J, sort = stages))p95<-pop.projection(proj95, n, it)(l95<-p95$lambda)  # lambda is much lower in 1995(p11<-pop.projection(proj11, n, pi))(l11<-p11$lambda)  # and based on 2011 alone extinction is eminent. The gist here is we need lots of years of data to make any decent estimation                   # of what the population is really likely to do (ie more than the three here)
#####################################################################################thesearethemeanprojmats<-list(proj94, proj95, proj11)  # make a list of the three matrices(meanxprojmat <-mean(thesearethemeanprojmats))   # make a mean of the three projection matrices for deterministic analysisn         # recall what n is, our starting population vector, ie the # of individuals in each class at the start of the projection(pprojme <- pop.projection(meanxprojmat, n))   # do the deterministic projection, lambda is the dominant left eigenvalue(DetLamb<-pprojme$lambda)
thesearethemeanprojmats # our list of projection matricesstochme <- stoch.growth.rate(thesearethemeanprojmats, prob=NULL, maxt=50000, verbose=TRUE)#  note that these stochastic approximations of lambda are in log form  (not immediately comparable to pop.project$lambda)exp(stochme$approx) # is the analytic approximation of lambda via Tuljapakar's methodstochme$approx      # this is more accurate (perhaps) when there is a lot of covariation in matrix elementsexp(stochme$sim)    # gives stochastic growth rate by simulation, random draws of wholestochme$sim
yearweight<-c(1,1,2)moredrought <- stoch.projection(thesearethemeanprojmats, n, tmax=50, prob=yearweight,  nreps = 500)# the output is population sizes, which are fun to graph when comparing modelsyearweight<-c(1,1,0)nodrought <- stoch.projection(thesearethemeanprojmats, n, tmax=50, prob=yearweight,  nreps = 500)
par(mfrow=c(2,1))hist(log(apply(moredrought, 1, sum)),col="blue",density=50, ylim=c(0,150),      xlim=c(-1.3,25), xlab="", main='More drought')abline(v=log10(200), lty=3)  # puts a line at the starting population size for referencehist(log(apply(nodrought, 1, sum)),col="green3",density=50, ylim=c(0,150),      xlim=c(-1.3,25), xlab="", main='No drought')abline(v=log10(200), lty=3)
obsd<-stoch.quasi.ext(thesearethemeanprojmats, n,prob=c(1,1,1),                    Nx=10, tmax = 50, maxruns = 10, nreps=500, sumweight=c(1,1,1))drt<-stoch.quasi.ext(thesearethemeanprojmats, n,prob=c(1,1,2),                    Nx=10, tmax = 50, maxruns = 10, nreps=500, sumweight=c(1,1,1))par(mfrow=c(2,1))matplot(obsd, ylab="Quasi-extinction probability",  ylim=c(0,1.1),        type="l", lty=1, col=rainbow(10), las=1, main='Observed climate',        xlab="Years")matplot(drt, ylab="Quasi-extinction probability",  ylim=c(0,1.1),        type="l", lty=1, col=rainbow(10), las=1, main='Double drought',        xlab="Years")
meanxprojmat  # for an overall look at sensitivity and elasticity use the mean projection matrix# you could do separate analyses by year or type of year too to examine how sensitivity and elasticity # vary among years(eigout <- eigen.analysis(meanxprojmat))  # do the associated sensitivity analysiscolSums(eigout$sensitivities)  # this gives the cumulative sensitivity of each stage/class, fun to graph# calculate fertility and survival sums(projsums <- colSums(meanxprojmat))(fert_row <- meanxprojmat[1,])(surv_row <- projsums-fert_row)# can do this for sens and elas too to make a bar chart # make x where columes are name, sens and elas# for my data I'm summing elasticity and sensitivity for each class.  You could also pick the vital # rates that are most meaningful in your own analysis(eigout <- eigen.analysis(meanxprojmat))eigout$sensitivities(fert_row_s <- eigout$sensitivities[1,])(surv_row_s <-eigout$sensitivities[2,] + eigout$sensitivities[3,] +eigout$sensitivities[4,] + eigout$sensitivities[5,] )sensme<-t(cbind(fert_row_s, surv_row_s))#
(fert_row_e <- eigout$elasticities[1,])(surv_row_e <-eigout$elasticities[2,] + eigout$elasticities[3,] +eigout$elasticities[4,] + eigout$elasticities[5,] )(survtable<-t(rbind(surv_row_s, surv_row_e)))(ferttable<-t(rbind(fert_row_s, fert_row_e)))par(mfrow=c(2,2))mynames <- c("Sensitivity", "Elasticity")barplot(t(survtable[,1:2]), beside=TRUE, las=1, ylim=c(0, 3.5),xlab="Stage class",        main="Growth and survival")abline(h=0)barplot(t(ferttable[,1:2]), beside=TRUE, las=1, ylim=c(0, .25), xlab="Stage class",        main="Fertility")abline(h=0)legend("topright", mynames, fill=grey.colors(2))
andre <-read.csv ("/Users/ctg/Dropbox/Projects/ipms/Teaching/Matrix_Models_Intro/Exercises/D__composite9_13_2012.csv")
str(andre)#  BEST ESTIMATE OF SEED PRODUCTION:  average of seeds/fruit, weighed average of fruits/infl where 0.14 is 1/8 drought yearsandre$SEEDS<-andre$MEDIAN_INFL * 14.35  # = 14.35 seeds/infl# The big crux Part 2:  How many seeds makes a J plant the next year? # ie what is the transition rate or fecundity rate?# observed juveniles in each year (sadly not all years have the same # of cohorts, so I adjust for that below) seedling_yr<-ddply (andre, c("YEAR"), function (df)  return(c(sdlgs=sum(df$CLASS == "J"))))    # shows the OBSERVED # of J plants each year, # and some of these are not observation years and are omitted belowseedling_yr#seedlings for 9 cohorts based on census from each year sdl94=seedling_yr$sdlgs[(seedling_yr$YEAR=="1994")]*9/2  # this year had only 2 cohortssdl95=seedling_yr$sdlgs[(seedling_yr$YEAR=="1995")]sdl11=seedling_yr$sdlgs[(seedling_yr$YEAR=="2011")]sdl12=0     # in 2012 observed seedlings were 0seedling_pick=c(sdl94,sdl95,sdl11, sdl12)seedling_pick# estimate seedlings as the mean of the 4 OBSERVED years:# in the other years seedlings where not surveyed for(seedlings=mean(seedling_pick))  # = 125.375 seedlings/year on average # what's the annual total seed production rate?# add up all the seeds estimated to be produced in each yearseed_yr<-ddply (andre, c("YEAR"), function (df)  return(c(sumseeds=sum(df$SEEDS))))seed_yr# adjust so that 1994 has an estimate of all cohorts based on the observed 2 cohortsseed_yr$sumseeds[(seedling_yr$YEAR=="1994")]<-(seed_yr$sumseeds[(seedling_yr$YEAR=="1994")]*9/2)# get mean seeds/yearavg_seeds_p_yr<-mean(seed_yr$sumseeds)# for each of the 4 years in which seedlings were observed, calculate an estimate of the transition rate from seed --> Jstr(sdl.trans94<-sdl94/avg_seeds_p_yr)  str(sdl.trans95<-sdl95/avg_seeds_p_yr)   str(sdl.trans11<-sdl11/avg_seeds_p_yr)   sdl.trans12<-0  # no seedlings observed this year# looks at the options for transition ratestr(seedlingtrans_pick<-c(sdl.trans94,sdl.trans95,sdl.trans11,sdl.trans12))# pick the mean for this analysis (seedlingtrans<-mean(seedlingtrans_pick))       # mean seedling transition rate  0.00305### Now remove watered and caged plants from main dataset, these have different survival and transition rates.# I included them thus far because we needed to get seed production estimates for them. Since they might have contributed# to the observed juveniles andre<-andre[(andre$CAGED =="N"),]  andre<-andre[(andre$WATERED =="N"),]head2(andre)  # reduce datafile to include only PLANT_UNQ, YEAR, CLASS and SEEDSstr(andre)str(andre<-andre[,c(1:3,16)])# Go back to step 2.
andre <-read.csv ("~/Dropbox/Projects/ipms/Teaching/Matrix_Models_Intro/Exercises/karadat.csv")str(andre)# Let's look at the stages/classeslevels(andre$CLASS)  # note that you need "dead" as a class for the first year that
str(andre)
head(andre)
str(andre)
return(table(df$CLASS))))
return(table(df$CLASS)))
(n_options<-ddply (andre, c("YEAR"), function (df) return(table(df$CLASS))))
n95<-c(81,31,17,13,11)n=n95  # this must be called "n" for popbio to work its magic
trans<-subset(merge(andre, andre, by = "PLANT_UNQ", sort = FALSE), YEAR.x == YEAR.y - 1)head2(trans)
rownames(trans) <- 1:nrow(trans)colnames(trans)[1:7] <- c("plant",  "year", "stage", "seeds",  "year2", "fate", "seeds2")head2(trans)
seedlingtrans<-0.00305    # This is the rate at which a seed becomes a J individual (I estimated this elsewhere, see Appendix below)
# adding in the number of J individuals produced by each individualtrans$J<- trans$seeds * seedlingtrans   # note that J is not an integer, which is totally fine, its a rate of J productionhead2(trans)
trans<-subset(merge(andre, andre, by = "PLANT_UNQ", sort = FALSE), YEAR.x == YEAR.y - 1)
trans
colnames(trans)[1:7] <- c("plant",  "year", "stage", "seeds",  "year2", "fate", "seeds2")
head2(trans)
colnames(trans) <- c("plant",  "year", "stage", "seeds",  "year2", "fate", "seeds2")
it<-100        # set the number of time steps for a deterministic model# Make a demographic projection matrix for each year like so:#     1994  trans94 <-subset(trans, year == 1994, c(plant, stage, fate, J))(proj94<-projection.matrix(trans94, stage, fate, J, sort = stages))  #this gives you
(p94<-pop.projection(proj94, n, it))
(l94<-p94$lambda)   # wow! if we looked only at 1994 based on these estimates the population would be booming!
l94$stable.stage
l94$stable.stage
p94$stable.stage
(l94<-p94$lambda)   # wow! if we looked only at 1994 based on these estimates the population would be booming!
#     1995  trans95 <-subset(trans, year == 1995, c(plant, stage, fate, J))(proj95<-projection.matrix(trans95, stage, fate, J, sort = stages))#     2011  trans11 <-subset(trans, year == 2011, c(plant, stage, fate, J))(proj11<-projection.matrix(trans11, stage, fate, J, sort = stages))p95<-pop.projection(proj95, n, it)(l95<-p95$lambda)  # lambda is much lower in 1995(p11<-pop.projection(proj11, n, pi))(l11<-p11$lambda)  # and based on 2011 alone extinction is eminent. The gist here is
thesearethemeanprojmats<-list(proj94, proj95, proj11)  # make a list of the three matrices(meanxprojmat <-mean(thesearethemeanprojmats))   # make a mean of the three projection matrices for deterministic analysisn         # recall what n is, our starting population vector, ie the # of individuals in each class at the start of the projection(pprojme <- pop.projection(meanxprojmat, n))   # do the deterministic projection, lambda is the dominant left eigenvalue(DetLamb<-pprojme$lambda)
thesearethemeanprojmats # our list of projection matricesstochme <- stoch.growth.rate(thesearethemeanprojmats, prob=NULL, maxt=50000, verbose=TRUE)#  note that these stochastic approximations of lambda are in log form  (not immediately comparable to pop.project$lambda)exp(stochme$approx) # is the analytic approximation of lambda via Tuljapakar's methodstochme$approx      # this is more accurate (perhaps) when there is a lot of
meanxprojmat  # for an overall look at sensitivity and elasticity use the mean projection matrix
(eigout <- eigen.analysis(meanxprojmat))  # do the associated sensitivity analysis
colSums(eigout$sensitivities)  # this gives the cumulative sensitivity of each stage/class, fun to graph
# calculate fertility and survival sums(projsums <- colSums(meanxprojmat))(fert_row <- meanxprojmat[1,])(surv_row <- projsums-fert_row)
(eigout <- eigen.analysis(meanxprojmat))
eigout$sensitivities
(fert_row_s <- eigout$sensitivities[1,])(surv_row_s <-eigout$sensitivities[2,] + eigout$sensitivities[3,] +eigout$sensitivities[4,] + eigout$sensitivities[5,] )
sensme<-t(cbind(fert_row_s, surv_row_s))
sensme
(fert_row_e <- eigout$elasticities[1,])(surv_row_e <-eigout$elasticities[2,] + eigout$elasticities[3,] +eigout$elasticities[4,] + eigout$elasticities[5,] )(survtable<-t(rbind(surv_row_s, surv_row_e)))(ferttable<-t(rbind(fert_row_s, fert_row_e)))
par(mfrow=c(2,2))mynames <- c("Sensitivity", "Elasticity")barplot(t(survtable[,1:2]), beside=TRUE, las=1, ylim=c(0, 3.5),xlab="Stage class",        main="Growth and survival")abline(h=0)barplot(t(ferttable[,1:2]), beside=TRUE, las=1, ylim=c(0, .25), xlab="Stage class",        main="Fertility")abline(h=0)legend("topright", mynames, fill=grey.colors(2))
par(mfrow=c(1,2))mynames <- c("Sensitivity", "Elasticity")barplot(t(survtable[,1:2]), beside=TRUE, las=1, ylim=c(0, 3.5),xlab="Stage class",        main="Growth and survival")abline(h=0)barplot(t(ferttable[,1:2]), beside=TRUE, las=1, ylim=c(0, .25), xlab="Stage class",        main="Fertility")abline(h=0)legend("topright", mynames, fill=grey.colors(2))
andre <-read.csv ("~/Beginner/Matrix_Models_Intro/Exercises/karadat.csv")
setwd('~/Dropbox/Projects/ipms/teachIPMs') # set this to your teachIPMs directory
andre <-read.csv ("~/Beginner/Matrix_Models_Intro/Exercises/karadat.csv")
andre <-read.csv ("Beginner/Matrix_Models_Intro/Exercises/karadat.csv")
andre <-read.csv ("Beginner/Intro_to_Matrix_Models/Exercises/karadat.csv")
colSums(eigout$elasticities)  # this gives the cumulative sensitivity of each stage/class
sum(colSums(eigout$elasticities))  # this gives the cumulative sensitivity of each stage/class
(projsums <- colSums(meanxprojmat))(fert_row <- meanxprojmat[1,]) # expected number of juveniles from an individual in each age class(surv_row <- projsums-fert_row) # survival probability of an individual in each age class
(eigout <- eigen.analysis(meanxprojmat))eigout$sensitivities(fert_row_s <- eigout$sensitivities[1,])(surv_row_s <-eigout$sensitivities[2,] + eigout$sensitivities[3,] +eigout$sensitivities[4,] + eigout$sensitivities[5,] )sensme<-t(cbind(fert_row_s, surv_row_s))
eigout$elasticities
(surv_row_e <-eigout$elasticities[2,] + eigout$elasticities[3,]
(surv_row_e <-eigout$elasticities[2,] + eigout$elasticities[3,] +eigout$elasticities[4,] + eigout$elasticities[5,] )
(surv_row_e <-eigout$elasticities[2,] + eigout$elasticities[3,] +eigout$elasticities[4,] + eigout$elasticities[5,] )
(surv_row_e <-eigout$elasticities[2,] + eigout$elasticities[3,] +eigout$elasticities[4,] + eigout$elasticities[5,] )
(surv_row_e <-apply(eigout$elasticities[2:5,],2,sum)
(surv_row_e <-apply(eigout$elasticities[2:5,],2,sum))
(surv_row_e <-apply(eigout$elasticities[2:5,],2,sum))
(surv_row_e <-apply(eigout$elasticities[2:5,],2,sum))
)
(surv_row_e <-apply(eigout$elasticities[2:5,],2,sum))
(surv_row_s <-apply(eigout$sensitivities[2:5,],2,sum))
+eigout$sensitivities[4,] + eigout$sensitivities[5,] )
(surv_row_s <-eigout$sensitivities[2,] + eigout$sensitivities[3,] +eigout$sensitivities[4,] + eigout$sensitivities[5,] )
(survtable<-t(rbind(surv_row_s, surv_row_e)))
t(survtable[,1:2])
barplot(t(survtable), beside=TRUE, las=1, ylim=c(0, 3.5),xlab="Stage class",
barplot(t(survtable), beside=TRUE, las=1, ylim=c(0, 3.5),xlab="Stage class",main="Growth and survival")
barplot(t(survtable), beside=TRUE, las=1, ylim=c(0, 3.5),xlab="Stage class",main="Growth and survival")
barplot(t(survtable), beside=TRUE, las=1, ylim=c(0, 3.5),xlab="Stage class",main="Growth and survival")
barplot(t(ferttable), beside=TRUE, las=1, ylim=c(0, .25), xlab="Stage class",main="Fertility")
abline(h=0)
par(mfrow=c(1,2))barplot(t(survtable), beside=TRUE, las=1, ylim=c(0, 3.5),xlab="Stage class",main="Growth and survival")barplot(t(ferttable), beside=TRUE, las=1, ylim=c(0, .25), xlab="Stage class",main="Fertility")legend("topright", c("Sensitivity", "Elasticity"), fill=grey.colors(2))
eigout$elasticities
eigout$sensitivities*meanxprojmat
sum(eigout$sensitivities*meanxprojmat)
eigout$sensitivities*meanxprojmat/sum(eigout$sensitivities*meanxprojmat)
(fert_row_e <- eigout$elasticities[1,])(surv_row_e <-apply(eigout$elasticities[2:5,],2,sum)) (survtable<-t(rbind(surv_row_s, surv_row_e)))(ferttable<-t(rbind(fert_row_s, fert_row_e)))
surv_row_e
t(survtable)
par(mfrow=c(1,2))barplot(surv_row_e, beside=TRUE, las=1, ylim=c(0, 3.5),xlab="Stage class",main="Growth and survival")barplot(fert_row_e , beside=TRUE, las=1, ylim=c(0, .25), xlab="Stage class",main="Fertility")
barplot(surv_row_e, beside=TRUE, las=1,xlab="Stage class",main="Elasticity for Growth and Survival")
barplot(surv_row_e,xlab="Stage class",main="Elasticity for Growth and Survival")
barplot(fert_row_e, xlab="Stage class",main="Elasticity for Fertility")
par(mfrow=c(1,2))barplot(surv_row_e,xlab="Stage class",main="Elasticity for Growth and Survival")barplot(fert_row_e, xlab="Stage class",main="Elasticity for Fertility")
setwd('~/Dropbox/Projects/ipms/teachIPMs') # set this to your teachIPMs directory
source("Advanced/Range_Models/Exercises/setup_invasive_ipms.r")
d.temp=d[complete.cases(d[,c('sizeNext','size')]),]
d.temp=d[complete.cases(d[,c('sizeNext','size')]),]gr.form= sizeNext~size+PAR+N+Ph.ave+mt.warm.month+mp.maygr.reg=MCMCglmm(gr.form, data=d.temp, verbose=FALSE,burnin=3000, nitt=13000,thin=10)summary(gr.reg)
d=read.csv('Advanced/Range_Models/Exercises/garlic_mustard_data_frame_2_18.csv')mean.envs=read.csv('Advanced/Range_Models/Exercises/garlic_mustard_plot_attributes.csv')minSize=4 maxSize=15
setwd('~/Dropbox/Projects/ipms/teachIPMs') # set this to your teachIPMs directory
d=read.csv('Advanced/Range_Models/Exercises/garlic_mustard_data_frame_2_18.csv')mean.envs=read.csv('Advanced/Range_Models/Exercises/garlic_mustard_plot_attributes.csv')minSize=4 maxSize=15
source("Advanced/Range_Models/Exercises/Setup_Range_IPMs.r")
source("Advanced/Range_Models/Exercises/Setup_Range_IPMs.r")
d=read.csv('Advanced/Range_Models/Exercises/garlic_mustard_data_frame_2_18.csv')mean.envs=read.csv('Advanced/Range_Models/Exercises/garlic_mustard_plot_attributes.csv')minSize=4 maxSize=15
d.temp=d[complete.cases(d[,c('sizeNext','size')]),]gr.form= sizeNext~size+PAR+N+Ph.ave+mt.warm.month+mp.maygr.reg=MCMCglmm(gr.form, data=d.temp, verbose=FALSE,burnin=3000, nitt=13000,thin=10)summary(gr.reg)
save(gr.reg,file='Model_Output/AP_growth_v5.post')
setwd('~/Dropbox/Projects/ipms/teachIPMs/Advanced/Range_Models/Exercises/') # set this to your teachIPMs directory
source("Setup_Range_IPMs.r")
d=read.csv('garlic_mustard_data_frame_2_18.csv')
mean.envs=read.csv('garlic_mustard_plot_attributes.csv')
d.temp=d[complete.cases(d[,c('sizeNext','size')]),]
gr.form= sizeNext~size+PAR+N+Ph.ave+mt.warm.month+mp.may
gr.reg=MCMCglmm(gr.form, data=d.temp, verbose=FALSE,burnin=3000, nitt=13000,thin=10)
summary(gr.reg)
save(gr.reg,file='Model_Output/AP_growth_v5.post')
plot((gr.reg$Sol)) # for random effects
summary(gr.reg)
plot((gr.reg$Sol))
plot((gr.reg$Sol))
posterior.mode(gr.reg$Sol)
coda::HPDinterval(gr.reg$Sol, 0.95)
autocorr.plot(gr.reg$Sol)
gr.diagnostics(gr.reg,d.temp,random=FALSE)
# map of growth predictionspdf('Figures/AP_growth_map.pdf',h=5,w=10)par(mfrow=c(1,2),oma=c(0,0,2,4))which.subset=mean.envs$habitat==0newdata=data.frame(size=quantile(d$size,.01,na.rm=T), PAR=mean(mean.envs$PAR[which.subset]),N=mean(mean.envs$N[which.subset]),Ph.ave=mean(mean.envs$Ph.ave[which.subset]),sm.t1=mean(mean.envs$sm.t1[which.subset]),values(ne.env)[not.nas,])gr.map(gr.reg,d,'AP\nclosed\ncanopy',newdata,zlims=c(0,2))which.subset=mean.envs$habitat==1newdata=data.frame(size=quantile(d$size,.01,na.rm=T), PAR=mean(mean.envs$PAR[which.subset]),N=mean(mean.envs$N[which.subset]),Ph.ave=mean(mean.envs$Ph.ave[which.subset]),sm.t1=mean(mean.envs$sm.t1[which.subset]),values(ne.env)[not.nas,])gr.map(gr.reg,d,'AP\nopen\ncanopy',newdata,zlims=c(0,2))dev.off()system("open Figures/AP_growth_map.pdf")
d.sv=d[complete.cases(d[,c('surv')]),]round(cor(d.sv[,best.var]),2)
d.sv=d[complete.cases(d[,c('surv')]),]#== checking for correlated predictors, as this model has trouble converging.round(cor(d.sv[,best.var]),2)sv.form=surv~size+PAR+N+Ph.ave+mt.warm.month+mp.maysv.reg=MCMClogit(sv.form, data=d.sv,b0=0, B0=.001,mcmc=5e4,thin=50)summary(sv.reg)save(sv.reg,file='Model_Output/Posteriors/AP_surv_v5.post')
save(sv.reg,file='Model_Output/AP_surv_v5.post')
sv.form=surv~size+PAR+N+Ph.ave+mt.warm.month+mp.maysv.reg=MCMClogit(sv.form, data=d.sv,b0=0, B0=.001,mcmc=5e4,thin=50)summary(sv.reg)save(sv.reg,file='Model_Output/AP_surv_v5.post')  # (load('Model_Output/AP_surv_v3.post'))#== explore output# posterior.mode(sv.reg$Sol)coda::HPDinterval(sv.reg, 0.95)autocorr.plot(sv.reg)plot((sv.reg)) sv.reg2=MCMClogit(sv.form, data=d.sv,b0=0, B0=.001,mcmc=5e4,thin=50,beta.start=runif(7,-5,5))summary(sv.reg2)sv.reg3=MCMClogit(sv.form, data=d.sv,b0=0, B0=.001,mcmc=5e4,thin=50,beta.start=runif(7,-5,5))summary(sv.reg3)sv.list=mcmc.list(sv.reg,sv.reg2,sv.reg3)gelman.plot(sv.list)gelman.diag(sv.list)
pdf('Figures/AP_sv_diagnostics.pdf',h=4,w=5)sv.diagnostics(sv.reg,d.sv,form=sv.form)dev.off()system("open Figures/AP_sv_diagnostics.pdf")
#== map of surv predictionspdf('Figures/AP_surv_map.pdf',h=5,w=10)par(mfrow=c(1,2),oma=c(0,0,2,4))which.subset=mean.envs$habitat==0newdata=data.frame(size=quantile(d$size,.5,na.rm=T), PAR=mean(mean.envs$PAR[which.subset]),N=mean(mean.envs$N[which.subset]),Ph.ave=mean(mean.envs$Ph.ave[which.subset]),sm.t1=mean(mean.envs$sm.t1[which.subset]),values(ne.env)[not.nas,],surv=1e4) sv.map(sv.reg,'AP\nclosed\ncanopy',newdata,form=sv.form,zlims=c(0,1))which.subset=mean.envs$habitat==1newdata=data.frame(size=quantile(d$size,.5,na.rm=T), PAR=mean(mean.envs$PAR[which.subset]),N=mean(mean.envs$N[which.subset]),Ph.ave=mean(mean.envs$Ph.ave[which.subset]),sm.t1=mean(mean.envs$sm.t1[which.subset]),values(ne.env)[not.nas,],surv=1e4) sv.map(sv.reg,'AP\nopen\ncanopy',newdata,form=sv.form,zlims=c(0,1))dev.off()system("open Figures/AP_surv_map.pdf")
d.seed=d[!is.na(d$fec1) & !is.na(d$size),]seed.form=fec1~size+PAR+Ph.ave+mt.warm.month+mp.mayfec1.reg=MCMCpoisson(seed.form, data=d.seed,b0=0, B0=.001,mcmc=5e4,thin=50)  summary(fec1.reg)#== B. % germinationd.germ=bernoullize(d[!is.na(d$fec2),],col.name1='n.germ.1', col.name0='n.germ.0')germ.form=new.1.0~Ph.ave+light+mt.warm.month+mp.mayfec2.reg=MCMClogit(germ.form, data=d.germ,b0=0, B0=.001,mcmc=5e4,thin=50)  summary(fec2.reg)#== C. germinant survivald.germ.s=bernoullize(d[!is.na(d$fec3),],col.name1='n.germ.surv.1', col.name0='n.germ.surv.0')sum.sv.form=new.1.0~Ph.ave+light+mt.warm.month+mp.mayfec3.reg=MCMClogit(sum.sv.form, data=d.germ.s,b0=0, B0=.001,mcmc=5e4,thin=50)summary(fec3.reg)#== D. germinant sized.off=subset(d, (d$Year.planted==d$Year.size) &  is.na(d$flowering))offspr.reg=MCMCglmm(size~1, data=d.off, burnin=3000,nitt=13000,thin=10,verbose=T)summary(offspr.reg)save(fec1.reg,fec2.reg,fec3.reg,offspr.reg, file='Model_Output/AP_fec_v5.post')# (load('Model_Output/AP fec 10-3-12.post'))
pdf('Figures/AP_fec_diagnostics.pdf',h=9,w=6)	fec.diagnostics()dev.off()system("open Figures/AP_fec_diagnostics.pdf")
pdf('Figures/AP_fec_map.pdf',h=5,w=15)par(mfrow=c(1,3),oma=c(0,0,2,4)) newdata=data.frame(size=quantile(d$size,.75,na.rm=T), PAR=1.5,N=-1.5,Ph.ave=1.5,light=1.5,values(ne.env)[not.nas,],new.1.0=1e5) seed.map(fec1.reg,'AP',newdata,"seed\nnumber",seed.form=seed.form, zlims=c(0,2000)) sv.map(fec2.reg,'AP',newdata,label="germination\nprobability", form=germ.form) sv.map(fec3.reg,'AP',newdata,label="germinant\nsurvival\nprobability", form=sum.sv.form)dev.off()system("open Figures/AP_fec_map.pdf")
d.fl=d[!is.na(d$flowering) & !is.na(d$size),]fl.form=flowering~size+I(size^2)+I(size^3)+PAR+N+Ph.ave+mp.may+mt.warm.monthfl.reg=MCMClogit(fl.form, data=d.fl,b0=0, B0=.001,mcmc=5e4,thin=50)summary(fl.reg)save(fl.reg,file='Model_Output/AP_flowering_v5.post')
autocorr.plot(fl.reg)plot((fl.reg))
plot((fl.reg))
pdf('Figures/AP_fl_diagnostics.pdf',h=6,w=6)	fl.diagnostics(random=FALSE,form=fl.form)dev.off()system("open Figures/AP_fl_diagnostics.pdf")
pdf('Figures/AP_fl_map.pdf',h=5,w=5) newdata=data.frame(size=quantile(d$size,.75,na.rm=T), PAR=1.5,N=-1.5,Ph.ave=1.5,values(ne.env)[not.nas,],flowering=1e4) sv.map(fl.reg,'AP\nopen canopy',newdata,label='flowering\nprobability',form=fl.form)dev.off()system("open Figures/AP_fl_map.pdf")
pdf('Figures/AP_all_response_curves_mt_warm_month.pdf',w=4,h=12)	response.summary(mean.envs,grad='mt.warm.month',lab=letters[1:6],'Mean Temperature Warmest Month')dev.off()system("open Figures/AP_all_response_curves_mt_warm_month.pdf")
pdf('Figures/AP_all_response_curves_mp_may.pdf',w=4,h=12)	response.summary(mean.envs,grad='mp.may',lab=letters[1:6],"Mean May Precipitation")dev.off()system("open Figures/AP_all_response_curves_mp_may.pdf")
pop.map
#== set up splitting for parallelizingntasks=7#== open or closed habitathabitat=1 #1=open; 0=closed#== managementmanage='' # for bakcward compatibility#manage=.02 # needed for south open and closed#== file labellabel=paste0('NE_habitat',habitat,manage)version='v6'#== specify the environmental conditionsenv.subset=as.matrix(data.frame(unique.env, PAR=ifelse(habitat==1,1.5,-1),N=1.5,Ph.ave=1.5,light=ifelse(habitat==1,1.5,-1)))cell.trick=env.cell.id#== set up indices for cells being useduse=1:nrow(env.subset)(ncells=length(use))#== set up paralleltask.split=chop.tasks(use,ntasks)          #== number of IPM cellsn.matrix = 50b=minSize+c(0:n.matrix)*(maxSize-minSize)/n.matrix #== mesh points (midpoints of the cells)y=0.5*(b[1:n.matrix]+b[2:(n.matrix+1)])#== width of the cellsh=y[2]-y[1]
head(env)
#== specify formulas for use in vital rate functionsgr.form=as.formula(paste('~',as.character(gr.reg$Fixed$formula)[3]))sv.form=as.formula(paste('~',paste0(dimnames(sv.reg)[[2]][-1],collapse='+')))seed.form=as.formula(paste('~',paste0(dimnames(fec1.reg)[[2]][-1],collapse='+')))germ.form=as.formula(paste('~',paste0(dimnames(fec2.reg)[[2]][-1],collapse='+')))sum.sv.form=as.formula(paste('~',paste0(dimnames(fec3.reg)[[2]][-1],collapse='+')))fl.form=as.formula(paste('~',paste0(dimnames(fl.reg)[[2]][-1],collapse='+')))offspr.form=as.formula(paste('~',as.character(offspr.reg$Fixed$formula)[3]))
gr.params.mean=apply(gr.reg$Sol,2,mean) # growthgr.params.sd=mean(gr.reg$VCV[,'units']) # growth variancesv.params=apply(sv.reg,2,mean)     #seed.params=apply(fec1.reg,2,mean)germ.params=apply(fec2.reg,2,mean)sum.sv.params=apply(fec3.reg,2,mean)offspr.params.mean=apply(offspr.reg$Sol,2,mean) # recruit sizeoffspr.param.sd=mean(offspr.reg$VCV[,'units']) # recruit size variancefl.params=apply(fl.reg,2,mean)
registerDoParallel(ntasks)#== GROWTH KERNEL ------------------------------------------------------- p.post.mean=foreach(i = 1:ntasks,.packages="Matrix") %dopar% {		p.mustard.kernel(task.split[[i]])}p.post.mean=unlist(p.post.mean,recursive=F)
f.post.mean=foreach(i = 1:ntasks,.packages="Matrix") %dopar% {		f.mustard.kernel(task.split[[i]],manage=manage)}f.post.mean=unlist(f.post.mean,recursive=F)
f.mustard.kernel
f.mustard.kernel=function(task.split,manage='',random=TRUE){	if(manage=='') manage=1	 post.temp=list(F=lapply(1:length(task.split), function(x) 1))	 for(k in 1:length(task.split)){		 ind=task.split[k]		 tenv=data.frame(t(env.subset[ind,]),new.1.0=1e4)		 offspr.size=offspr2(size=y,mean.params=offspr.params.mean, sd.param=offspr.param.sd, tenv,offspr.form)		 #offspr.size=matrix(rep(offspr2(size=y,mean.params=offspr.params.mean, sd.param=offspr.param.sd, tenv,offspr.form),n.matrix),byrow=T,nrow=n.matrix)		 #tf=t(matrix(rep(sv2(y,fl.params,tenv,fl.form,random=random),n.matrix), byrow=T,nrow=n.matrix))		 tf=manage*matrix(rep(sv2(y,fl.params,tenv,fl.form,random=random), n.matrix), byrow=T,nrow=n.matrix)		 #tsh=t(matrix(rep(seed2(y,seed.params,tenv,seed.form,random=TRUE),n.matrix), byrow=T,nrow=n.matrix))		 tsh=matrix(rep(seed2(y,seed.params,tenv,seed.form,random=random), n.matrix), byrow=T,nrow=n.matrix)		 germ=germ2(germ.params,tenv,germ.form,random=random)		 sum.sv=sum.sv2(sum.sv.params,tenv,sum.sv.form,random=random)		 #F=matrix(h*germ*sum.sv,n.matrix,n.matrix)*(tf*(tsh*offspr.size))		 F=h*offspr.size*germ*sum.sv*tf*tsh		 post.temp[[k]]=shrink.matrix(F,1e-5)	 }	 post.temp}
f.post.mean=foreach(i = 1:ntasks,.packages="Matrix") %dopar% {		f.mustard.kernel(task.split[[i]],manage=manage)}
f.post.mean=unlist(f.post.mean,recursive=F)
sim=foreach(i = 1:ntasks,.packages=c("Matrix","IPMpack")) %dopar% { dynamics.mustard(task.split[[i]])}
dynamics.mustard=function(task.split,useful.calcs=FALSE){	#==define things to save	lam=r0=sens1=elas1=passage.time=stable=r.val=le= list(lapply(1:length(task.split), function(i) 1))	for(i in 1:length(task.split)){		ind=task.split[i]  		#== for age 		IPM=matrix(0,2*n.matrix,2*n.matrix)		#old way		# IPM[ 1:n.matrix, 1:n.matrix ]=apply(p.post.mean[[ind]],2,rev) 		# IPM[ (n.matrix+1):(2*n.matrix) , (n.matrix+1):(2*n.matrix) ] =as.matrix(t(f.post.mean[[ind]]))		IPM[(n.matrix+1):(2*n.matrix), 1:n.matrix ]=as.matrix((p.post.mean[[ind]]))		IPM[  1:n.matrix, (n.matrix+1):(2*n.matrix)] =as.matrix((f.post.mean[[ind]]))		#image(IPM^.1)		lam[[i]]=max(Re(eigen(IPM)$values))		if(useful.calcs){			r0[[i]]=R0Calc(as.matrix(p.post.mean[[ind]]),as.matrix(f.post.mean[[ind]]))			sens1[[i]]=sens(IPM)			elas1[[i]]=elas(IPM)		}		if(maybe.useful.calcs){			#passage.time[[i]]=passageTime(8,as.matrix(p.post.mean[[ind]]))	      stable[[i]]=abs(Re(eigen(IPM)$vectors[, 1])) 			r.val[[i]]=abs(Re(eigen(t(IPM))$vectors[, 1])) 			le[[i]]=meanLifeExpect(as.matrix(p.post.mean[[ind]]))		}	}	list(lam=lam,r0=r0,sens1=sens1,elas1=elas1,passage.time=passage.time, stable=stable,r.val=r.val,le=le)}
sim=foreach(i = 1:ntasks,.packages=c("Matrix","IPMpack")) %dopar% { dynamics.mustard(task.split[[i]])}
dynamics.mustard=function(task.split,useful.calcs=FALSE,maybe.useful.calcs=FALSE){	#==define things to save	lam=r0=sens1=elas1=passage.time=stable=r.val=le= list(lapply(1:length(task.split), function(i) 1))	for(i in 1:length(task.split)){		ind=task.split[i]  		#== for age 		IPM=matrix(0,2*n.matrix,2*n.matrix)		#old way		# IPM[ 1:n.matrix, 1:n.matrix ]=apply(p.post.mean[[ind]],2,rev) 		# IPM[ (n.matrix+1):(2*n.matrix) , (n.matrix+1):(2*n.matrix) ] =as.matrix(t(f.post.mean[[ind]]))		IPM[(n.matrix+1):(2*n.matrix), 1:n.matrix ]=as.matrix((p.post.mean[[ind]]))		IPM[  1:n.matrix, (n.matrix+1):(2*n.matrix)] =as.matrix((f.post.mean[[ind]]))		#image(IPM^.1)		lam[[i]]=max(Re(eigen(IPM)$values))		if(useful.calcs){			r0[[i]]=R0Calc(as.matrix(p.post.mean[[ind]]),as.matrix(f.post.mean[[ind]]))			sens1[[i]]=sens(IPM)			elas1[[i]]=elas(IPM)		}		if(maybe.useful.calcs){			#passage.time[[i]]=passageTime(8,as.matrix(p.post.mean[[ind]]))	      stable[[i]]=abs(Re(eigen(IPM)$vectors[, 1])) 			r.val[[i]]=abs(Re(eigen(t(IPM))$vectors[, 1])) 			le[[i]]=meanLifeExpect(as.matrix(p.post.mean[[ind]]))		}	}	list(lam=lam,r0=r0,sens1=sens1,elas1=elas1,passage.time=passage.time, stable=stable,r.val=r.val,le=le)}
sim=foreach(i = 1:ntasks,.packages=c("Matrix","IPMpack")) %dopar% { dynamics.mustard(task.split[[i]])}
sim
str(maybe.useful.calcs)
str(sim)
n.lam=unlist(lapply(sim,function(x) x[['lam']]),recursive=FALSE)
n.lam
n.r0=unlist(lapply(sim,function(x) x[['r0']]),recursive=FALSE)
n.r0
n.lam
n.sens
pdf(paste0('Figures/AP_lambda_map_',label,'_',version,'.pdf'),h=7,w=7) pop.map( unlist(n.lam)[cell.trick], paste0('AP\n',ifelse(habitat==1,'open','closed'), ' habitat'),'lambda',max.val=30)dev.off()system(paste0('open Figures/AP_lambda_map_',label,'_',version,'.pdf'))
pdf(paste0('Figures/AP_lambda_response_curves',label,'_' ,version,'.pdf'),h=3,w=6) par(mfrow=c(1,2),mar=c(5,4,3,1))plot(values(ne.env)[not.nas,'mt.warm.month'],unlist(n.lam)[cell.trick], pch=19,col= rgb(100,100,100,40,maxColorValue=255), las=1,bty='n',xlab='Mean Temp. Warmest Month',ylab='Lambda')d.tmp=data.frame(lam=c(unlist(n.lam)[cell.trick]),values(ne.env)[not.nas, c('mt.warm.month','mp.may','n.droughts.gs')])m1=gam(lam~s(mt.warm.month),data=d.tmp)xx=seq(min(values(ne.env)[,'mt.warm.month'],na.rm=T),max(values(ne.env)[, 'mt.warm.month'],na.rm=T),by=.05)lines(xx,predict(m1,data.frame(mt.warm.month=xx)),col='red1',lwd=3)plot(values(ne.env)[not.nas,'mp.may'],unlist(n.lam)[cell.trick],pch=19, col=rgb(100,100,100,40,maxColorValue=255), las=1,bty='n',xlab='Mean May Precip.',ylab='Lambda')m1=gam(lam~s(mp.may),data=d.tmp)xx=seq(min(values(ne.env)[,'mp.may'],na.rm=T),max(values(ne.env)[, 'mp.may'],na.rm=T),by=.05)lines(xx,predict(m1,data.frame(mp.may=xx)),col='red1',lwd=3)dev.off()system(paste0('open Figures/AP_lambda_response_curves_',label,'_',version,'.pdf'))
pdf(paste0('Figures/AP_lambda_response_curves',label,'_' ,version,'.pdf'),h=3,w=6) par(mfrow=c(1,2),mar=c(5,4,3,1))plot(values(ne.env)[not.nas,'mt.warm.month'],unlist(n.lam)[cell.trick], pch=19,col= rgb(100,100,100,40,maxColorValue=255), las=1,bty='n',xlab='Mean Temp. Warmest Month',ylab='Lambda')d.tmp=data.frame(lam=c(unlist(n.lam)[cell.trick]),values(ne.env)[not.nas, c('mt.warm.month','mp.may','n.droughts.gs')])m1=gam(lam~s(mt.warm.month),data=d.tmp)xx=seq(min(values(ne.env)[,'mt.warm.month'],na.rm=T),max(values(ne.env)[, 'mt.warm.month'],na.rm=T),by=.05)lines(xx,predict(m1,data.frame(mt.warm.month=xx)),col='red1',lwd=3)plot(values(ne.env)[not.nas,'mp.may'],unlist(n.lam)[cell.trick],pch=19, col=rgb(100,100,100,40,maxColorValue=255), las=1,bty='n',xlab='Mean May Precip.',ylab='Lambda')m1=gam(lam~s(mp.may),data=d.tmp)xx=seq(min(values(ne.env)[,'mp.may'],na.rm=T),max(values(ne.env)[, 'mp.may'],na.rm=T),by=.05)lines(xx,predict(m1,data.frame(mp.may=xx)),col='red1',lwd=3)dev.off()
system(paste0('open Figures/AP_lambda_response_curves_',label,'_',version,'.pdf'))
label
paste0('Figures/AP_lambda_response_curves',label,'_' ,version,'.pdf')
paste0('open Figures/AP_lambda_response_curves_',label,'_',version,'.pdf')
pdf(paste0('Figures/AP_lambda_response_curves_',label,'_' ,version,'.pdf'),h=3,w=6) par(mfrow=c(1,2),mar=c(5,4,3,1))plot(values(ne.env)[not.nas,'mt.warm.month'],unlist(n.lam)[cell.trick], pch=19,col= rgb(100,100,100,40,maxColorValue=255), las=1,bty='n',xlab='Mean Temp. Warmest Month',ylab='Lambda')d.tmp=data.frame(lam=c(unlist(n.lam)[cell.trick]),values(ne.env)[not.nas, c('mt.warm.month','mp.may','n.droughts.gs')])m1=gam(lam~s(mt.warm.month),data=d.tmp)xx=seq(min(values(ne.env)[,'mt.warm.month'],na.rm=T),max(values(ne.env)[, 'mt.warm.month'],na.rm=T),by=.05)lines(xx,predict(m1,data.frame(mt.warm.month=xx)),col='red1',lwd=3)plot(values(ne.env)[not.nas,'mp.may'],unlist(n.lam)[cell.trick],pch=19, col=rgb(100,100,100,40,maxColorValue=255), las=1,bty='n',xlab='Mean May Precip.',ylab='Lambda')m1=gam(lam~s(mp.may),data=d.tmp)xx=seq(min(values(ne.env)[,'mp.may'],na.rm=T),max(values(ne.env)[, 'mp.may'],na.rm=T),by=.05)lines(xx,predict(m1,data.frame(mp.may=xx)),col='red1',lwd=3)dev.off()system(paste0('open Figures/AP_lambda_response_curves_',label,'_',version,'.pdf'))
